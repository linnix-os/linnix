# Feature Distribution: Open Source vs Enterprise

## Current Enhancement: Process Table in Reasoner

### âœ… CORRECT - Stays in Open Source (linnix-opensource)

The process table feature we just added to `linnix-reasoner` correctly belongs in the open-source repository because:

1. **Aligns with Architecture**: Open source reasoner = "LLM client (BYO model)"
   - Fetches system data from cognitod `/system` and `/processes` endpoints
   - Uses sysinfo (cross-platform library) to get process details
   - Formats data nicely for LLM consumption
   - User brings their own LLM (OpenAI, local llama.cpp, etc.)

2. **No Proprietary Value**: 
   - Process enumeration using public sysinfo API âœ…
   - Basic table formatting âœ…
   - Generic LLM prompting âœ…
   - NO custom models, NO training, NO enterprise datasets âŒ

3. **Enhances OSS Value**:
   - Makes cognitod more useful out-of-the-box
   - Shows off eBPF telemetry capabilities
   - Encourages "BYO model" approach
   - Good demo for potential enterprise customers

## Distribution Strategy

### Open Source (linnix-opensource)
**License**: Apache-2.0 (moving from AGPL)
**Repo**: github.com/linnix-os/linnix

```
Components:
â”œâ”€â”€ cognitod/
â”‚   â”œâ”€â”€ eBPF loader
â”‚   â”œâ”€â”€ Process tracking
â”‚   â”œâ”€â”€ Local ILM (rules engine)
â”‚   â”œâ”€â”€ HTTP/SSE API
â”‚   â””â”€â”€ Handlers: JSONL, rules
â”‚
â”œâ”€â”€ linnix-ai-ebpf/
â”‚   â”œâ”€â”€ fork/exec/exit probes
â”‚   â”œâ”€â”€ CPU/mem telemetry
â”‚   â””â”€â”€ License: GPL-2.0 OR MIT
â”‚
â”œâ”€â”€ linnix-cli/
â”‚   â”œâ”€â”€ Event streaming
â”‚   â””â”€â”€ Process tree visualization
â”‚
â”œâ”€â”€ linnix-reasoner/  â† WE ARE HERE
â”‚   â”œâ”€â”€ LLM client (generic)
â”‚   â”œâ”€â”€ System snapshot fetching
â”‚   â”œâ”€â”€ Process table formatting âœ… NEW
â”‚   â””â”€â”€ BYO model (OpenAI, local, etc.)
â”‚
â”œâ”€â”€ insight_tool/ (BASIC)
â”‚   â”œâ”€â”€ Heuristics only
â”‚   â”œâ”€â”€ Schema validation
â”‚   â””â”€â”€ 50 example records
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ examples/ (50 samples)
â”‚   â””â”€â”€ schema/
â”‚
â””â”€â”€ configs/
    â”œâ”€â”€ linnix.toml
    â””â”€â”€ rules.yaml
```

### Enterprise (linnix-enterprise)
**License**: Proprietary/Commercial
**Repo**: Private

```
Components:
â”œâ”€â”€ training-platform/
â”‚   â”œâ”€â”€ web-ui/ (React dataset browser)
â”‚   â”œâ”€â”€ api-server/ (Python FastAPI)
â”‚   â””â”€â”€ worker/ (Celery training jobs)
â”‚
â”œâ”€â”€ insight_tool/ (FULL)
â”‚   â”œâ”€â”€ LLM adapter (multi-provider)
â”‚   â”œâ”€â”€ Dataset expansion (synthetic generation)
â”‚   â”œâ”€â”€ Production collection (PagerDuty/Slack/SIEM)
â”‚   â””â”€â”€ Quality scoring
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ training/ (661+ curated records) ğŸ’°
â”‚   â”œâ”€â”€ synthetic/ (generated)
â”‚   â””â”€â”€ validation/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.sh (Axolotl orchestration)
â”‚   â”œâ”€â”€ quick_train.sh (Unsloth)
â”‚   â”œâ”€â”€ build_500_dataset.sh
â”‚   â””â”€â”€ collect_incidents.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ linnix-qwen-v1/ (fine-tuned 7B) ğŸ’°
â”‚   â”œâ”€â”€ linnix-3b-distilled/ (H200-trained) ğŸ’°
â”‚   â””â”€â”€ customer-specific/
â”‚
â”œâ”€â”€ cloud-control-plane/
â”‚   â”œâ”€â”€ Multi-tenancy
â”‚   â”œâ”€â”€ Billing (Stripe)
â”‚   â””â”€â”€ Auth/RBAC
â”‚
â”œâ”€â”€ advanced-ilm/
â”‚   â”œâ”€â”€ Anomaly detection (ML-powered)
â”‚   â”œâ”€â”€ Auto-remediation
â”‚   â””â”€â”€ Root cause analysis
â”‚
â””â”€â”€ integrations/
    â”œâ”€â”€ ServiceNow
    â”œâ”€â”€ Jira
    â””â”€â”€ SIEM
```

## Key Distinctions

### Open Source Gets:
- âœ… Basic process table formatting
- âœ… Generic LLM client (BYO API key)
- âœ… Heuristic-based insights
- âœ… 50 example incident records
- âœ… Rule-based local ILM
- âœ… Schema validation
- âœ… Full eBPF telemetry collection

### Enterprise Gets:
- ğŸ’° 661+ curated training datasets
- ğŸ’° Fine-tuned models (7B, 3B distilled)
- ğŸ’° LLM-assisted dataset expansion
- ğŸ’° Production data collectors (PagerDuty, Slack, SIEM)
- ğŸ’° Training platform UI
- ğŸ’° ML-powered anomaly detection
- ğŸ’° Auto-remediation
- ğŸ’° Enterprise integrations

## File Locations

### Process Table Feature (Current Enhancement)

**File**: `linnix-opensource/linnix-reasoner/src/main.rs`
**Status**: âœ… Correctly placed in open source
**Reason**: Basic LLM client functionality

**Changes**:
- Added sysinfo dependency
- Process enumeration (top 5 CPU + top 5 memory)
- ASCII table formatting
- Enhanced LLM prompts to include table

### Distilled Model Files

**Training artifacts** (should be enterprise):
- âŒ `h200-distilled-model/` (5.8GB PyTorch) - Keep in enterprise
- âŒ `training_data_12k.jsonl` - Keep in enterprise
- âŒ Training scripts, Axolotl configs - Keep in enterprise

**Inference artifacts** (can be open source for demo):
- âœ… `linnix-3b-distilled-q5_k_m.gguf` (2.1GB) - Can share as demo model
- âœ… `serve_distilled_model.sh` - Open source (generic llama.cpp server)
- âœ… Integration docs - Open source

## Recommendations

### Immediate Actions:

1. âœ… **Keep current process table in open source** - Already correctly placed
2. âœ… **Keep distilled model serving scripts in open source** - Helps adoption
3. âŒ **Move training data to enterprise** - Create `.gitignore` entries:
   ```gitignore
   # Training artifacts (enterprise only)
   training_data_*.jsonl
   h200-distilled-model/
   distillation_*.py
   ```

4. âœ… **Share GGUF model as demo** - Include in releases or S3

### Documentation Updates:

**In Open Source README**:
```markdown
## Pre-trained Models (Optional)

Linnix provides a distilled 3B model for demo purposes:
- Model: linnix-3b-distilled (Q5_K_M GGUF, 2.1GB)
- Download: [GitHub Releases](...)
- Serving: `./serve_distilled_model.sh`

For production deployments and custom fine-tuned models, see [Linnix Enterprise](https://linnix.io/pricing).
```

**In Enterprise README**:
```markdown
## Model Training

This repository contains:
- 661+ curated incident datasets
- H200-trained 3B distilled model (PyTorch + GGUF)
- Axolotl/Unsloth training orchestration
- Customer-specific fine-tuning pipelines
```

## Summary

âœ… **Process table feature is correctly in open source**
- Generic process enumeration
- Basic table formatting  
- LLM client enhancement
- No proprietary training data or models

ğŸ’° **Enterprise contains the valuable IP**
- 661+ curated training records
- Fine-tuned models
- Training pipelines
- Production data collectors
- Advanced ML features

This follows the "give away the razor, sell the blades" model:
- Open source = Excellent eBPF telemetry + basic LLM client
- Enterprise = Training data, fine-tuned models, advanced features
